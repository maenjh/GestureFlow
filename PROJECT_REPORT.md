# GestureFlow 프로젝트 상세 보고서

## 1. 프로젝트 개요
**GestureFlow (Magic Hand Filter)**는 컴퓨터 비전 기술을 활용하여 사용자의 손 제스처를 실시간으로 인식하고, 이에 반응하여 비디오 필터를 적용하거나 사진을 촬영하는 인터랙티브 웹 애플리케이션입니다. 별도의 장비 없이 일반 웹캠과 웹 브라우저만으로 동작하며, 직관적인 제스처 인터페이스(NUI)를 제공하는 것을 목표로 합니다.

## 2. 시스템 아키텍처
이 프로젝트는 **Python**을 기반으로 하며, 다음과 같은 핵심 기술 요소들로 구성되어 있습니다.

*   **Backend (Flask)**: 웹 서버 역할을 수행하며, 비디오 스트리밍 처리, API 엔드포인트 제공, 이미지 저장 및 관리를 담당합니다.
*   **Computer Vision (OpenCV)**: 이미지 처리의 핵심 엔진으로, 웹캠 영상 캡처, 색상 변환, 엣지 검출(Canny), 이미지 합성 등의 작업을 수행합니다.
*   **AI/ML (MediaPipe Hands)**: 구글의 온디바이스 머신러닝 솔루션을 사용하여 실시간으로 손의 21개 랜드마크를 3차원 좌표로 추적합니다.
*   **Frontend (HTML/CSS/JS)**: 사용자에게 비디오 피드를 보여주고, 갤러리를 실시간으로 업데이트하며, 수동 촬영 요청을 처리합니다.

## 3. 핵심 기능 및 구현 원리

### 3.1 실시간 비디오 스트리밍
Flask의 `Response` 객체와 제너레이터 함수를 사용하여 **Multipart Response (`multipart/x-mixed-replace`)** 방식을 구현했습니다.
*   OpenCV로 캡처한 프레임을 JPEG로 인코딩합니다.
*   인코딩된 바이트 스트림을 HTTP 응답의 일부로 지속적으로 클라이언트에 전송하여 동영상처럼 보이게 합니다.

### 3.2 제스처 인식 알고리즘
MediaPipe가 제공하는 손가락 관절(Landmark) 좌표를 분석하여 제스처를 판별합니다.
1.  **손가락 펴짐 감지 (`count_fingers`)**:
    *   엄지: 손목과 엄지 끝의 거리를 계산하거나 x축 좌표 비교를 통해 펴짐 여부 판단.
    *   나머지 네 손가락: 손가락 끝(Tip)의 y좌표가 두 번째 관절(PIP)보다 아래에 있는지(화면상 위쪽) 확인.
2.  **제스처 분류 (`detect_gesture`)**:
    *   **Palm**: 5개 손가락이 모두 펴진 경우.
    *   **Fist**: 모든 손가락이 접힌 경우 (엄지 예외 처리 포함).
    *   **Peace**: 검지와 중지만 펴지고 나머지는 접힌 경우.

### 3.3 필터 적용 로직
인식된 제스처에 따라 OpenCV 함수를 사용하여 실시간으로 프레임을 변환합니다.
*   **Grayscale (Fist)**: `cv2.cvtColor`를 사용하여 BGR 이미지를 흑백으로 변환합니다.
*   **Sketch (Peace)**:
    1.  흑백 변환.
    2.  `cv2.Canny` 알고리즘으로 엣지(윤곽선) 검출.
    3.  `cv2.bitwise_not`으로 색상을 반전시켜 흰 배경에 검은 선 효과 구현.
*   **Normal (Palm)**: 원본 프레임 유지.

### 3.4 자동 촬영 및 갤러리 시스템
*   **타이머 로직**: 'Fist' 제스처가 감지되면 `time.time()`을 사용하여 3초 카운트다운을 시작합니다. 화면에 남은 시간을 오버레이로 표시합니다.
*   **이미지 저장**: 카운트다운 종료 시점의 프레임(`global_frame`)을 `output` 디렉토리에 타임스탬프 파일명으로 저장합니다.
*   **실시간 갤러리**: 프론트엔드에서 JavaScript `setInterval`을 사용하여 주기적으로 `/gallery` API를 호출, 새로운 이미지가 있는지 확인하고 DOM을 업데이트합니다.

## 4. 기술적 도전 과제 및 해결

### 4.1 웹캠 리소스 점유 문제
*   **문제**: Flask 서버가 재시작되거나 오류가 발생했을 때 웹캠 리소스(`cv2.VideoCapture`)가 제대로 해제되지 않아 "Camera busy" 오류가 발생함.
*   **해결**: `generate_frames` 함수 내에서 프레임 읽기 실패 시(`cap.read()` return False), 즉시 루프를 종료하지 않고 `cap.release()` 후 잠시 대기했다가 재연결을 시도하는 로직을 추가하여 안정성을 확보했습니다.

### 4.2 제스처 인식의 불안정성 (Flickering)
*   **문제**: 손이 애매한 위치에 있을 때 제스처가 빠르게 바뀌며 필터가 깜빡이는 현상.
*   **해결**: (현재 코드에는 단순 구현되어 있으나) 향후 `collections.deque` 등을 사용하여 최근 N 프레임의 제스처 중 최빈값을 사용하는 'Smoothing' 로직을 도입하거나, 제스처 전환에 쿨다운 시간을 두어 해결할 수 있습니다. 본 프로젝트에서는 인식 신뢰도 임계값(`min_detection_confidence`)을 0.7로 높여 오인식을 줄였습니다.

### 4.3 양손 인식 및 우선순위
*   **문제**: 두 손이 동시에 화면에 나올 때 어떤 필터를 적용할지 모호함.
*   **해결**: `max_num_hands=2`로 설정하여 양손을 모두 인식하고, 감지된 모든 제스처 리스트를 수집한 뒤 우선순위(Peace > Fist > Palm)를 두어 하나의 필터만 적용되도록 로직을 설계했습니다.

## 5. 향후 개선 사항
1.  **딥러닝 기반 제스처 분류**: 현재의 좌표 기반 휴리스틱 방식 대신, 손 랜드마크 데이터를 입력으로 하는 LSTM이나 분류 모델을 학습시켜 더 다양하고 복잡한 제스처(OK 사인, 하트 등)를 인식하도록 개선.
2.  **WebSocket 도입**: 현재의 HTTP Polling 방식 갤러리 업데이트를 WebSocket으로 변경하여 서버 부하를 줄이고 실시간성을 극대화.
3.  **클라이언트 사이드 처리**: OpenCV 연산을 서버가 아닌 클라이언트(WebAssembly, OpenCV.js)에서 처리하도록 하여 서버 비용 절감 및 반응 속도 개선.

## 6. 결론
GestureFlow 프로젝트는 Python의 강력한 라이브러리 생태계를 활용하여 짧은 시간 내에 완성도 높은 AI 웹 애플리케이션을 구축할 수 있음을 보여줍니다. 컴퓨터 비전 기술이 웹 기술과 결합되었을 때 사용자에게 얼마나 직관적이고 재미있는 경험을 제공할 수 있는지 확인하는 계기가 되었습니다.
